---
title: "MA5810: Introduction to Data Mining"
subtitle: "Week 5; Collaborate Session 1: Principal Components Analysis"
author: "Martha Cooper, PhD"
institute: "JCU Masters of Data Science"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_accent(
  base_color = "#045a8d",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("IBM Plex Mono")
)
```

## Housekeeping

+ Collaborate 1 = **Tuesday 6.45-8pm** (Martha)

+ Collaborate 2 = **Thursday 6.45-8pm** (Martina)

For my Collaborate Sessions, you can get the **slides & R code** for each week here:

**https://github.com/MarthaCooper/MA5810**

.center[

<img src="Pics/download_github.png" style="width: 50%" />

]


---

## Assessments

+ Assessment 2 is due on **Sunday 08/08/21**

---
## Subject: MA5810 Intro to Data Mining

MA5810 Learning Outcomes

1. Overview of Data Mining and Examples

2. Unsupervised data mining methods e.g. clustering and outlier detection;

3. **Unsupervised** and supervised **techniques for dimensionality reduction (Today = PCA)**;

4. Supervised data mining methods for pattern classification;

5. **Apply these concepts to real data sets using R (Today)**.

---

## Today's Goals

+ Understand the background behind Principal Components Analysis (PCA)

+ Understand the pros and cons of PCA
 
+ Apply PCA to real datasets using R


---

## Unsupervised Learning

A set of statistical tools to understand a set of features, $X_1,...Xp$, without having an associated response variable, $Y$, to predict.

Data visualization, identification of subgroups & dimensionality reduction

### Principal Components Analysis (PCA)

A technique for summarizing a large set of variables into a smaller number of representative variables that collectively explain most of the variation in the original set. 

*PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance*

---

## Why reduce dimensions?

.pull-left[

**Problems:**

+ Correlated variables

+ A large number of variables

.center[

<img src="Pics/pca_probs.jpg" style="width: 100%" />

]

]

.pull-right[

**Solutions:**

+ A new set of variables that are *uncorrelated* and *explain as much variance as possible*

+ *The best combination* of all the variables that *explains the original data set with less variables*

.center[

<img src="Pics/pca_vis.jpg" style="width: 60%" />

]


]

---
## Finding the Prinicpal Components

+ Transform the data to a small number of **interesting** dimensions

+ **Interesting** = **Highest Variance**

+ These dimensions (**Principal Components**) are:

.pull-left[
    
##### 1. (Normalised) Linear combinations of the original variables

.center[

<img src="Pics/PC1.jpg" style="width: 120%" />

]

]

.pull-right[

##### 2. Uncorrelated
    
.center[

<img src="Pics/pc_orth.jpg" style="width: 70%" />

]   
]

---

## Finding the Prinicipal Components

+ How do we choose the loadings that cause PC1 to explain the most variance in the data? 

.center[

<img src="Pics/pc_proj.jpg" style="width: 80%" />

] 

---
## Finding the Prinicipal Components

+ How do we choose PC2 - the second biggest source of variation & uncorrelated?

.center[

<img src="Pics/pc2.jpg" style="width: 80%" />

] 

---

### Pros and Cons

.pull-left[

#### Pros

+ Reduce number of predictors
+ Reduce number of correlated predictors 
+ Identify subgroups in our dataset
+ Identify outliers

]

.pull-right[

#### Cons

+ Subjective
+ Exploratory data analysis
+ Difficult to interpret & assess results

]

---

## PCA in R
```{r, eval = F}

head(iris) #data
dat <- iris[ ,1:4] # remove Species column

pc <- prcomp(dat, center = T, scale = T) #why center? why scale?

pc$rotation #loadings - matrix of variable loadings
pc$x #scores - the coordinates of the observations on each PC

```

---

## Visualising PCA in R

```{r, eval = F}
library(factoextra)

fviz_screeplot(pc) #scree plot - proportion of variance explained
# How might we choose how many PCs to keep? 

fviz_pca_ind(pc) #PC1 vs PC2

fviz_pca_biplot(pc) #biplot
```

---

## Interpretting PCA with domain knowledge

```{r, eval = F}
fviz_pca_ind(pc, 
             habillage = iris$Species,
             addEllipses = TRUE)
```

---

## Scaling

```{r include=FALSE}
library(factoextra, quietly = T)
```
.center[


```{r echo=FALSE, fig.width = 7, fig.height = 3, dpi=100}
dat <- iris[ ,1:4] # remove Species column

pc_scaled <- prcomp(dat, center = T, scale = T) 
pc_unscaled <- prcomp(dat, center = T, scale = F)

a <- fviz_pca_biplot(pc_scaled, 
             habillage = iris$Species,
             addEllipses = TRUE,
             geom = "point")+
  ggtitle("Scaled")

b <- fviz_pca_biplot(pc_unscaled, 
             habillage = iris$Species,
             addEllipses = TRUE,
             geom = "point")+
  ggtitle("Unscaled")

egg::ggarrange(b,a, nrow=1)
```


```{r echo=FALSE}
data.frame(unscaled_variance = sapply(iris[1:4], var),
           scaled_variance = sapply(as.data.frame(scale(iris[1:4])), var))
```

]
---

## PCA in R

Different functions have different methods for calculating the principal components: 

+ prcomp() - Singular Value Decomposition (SVD)

+ factoextra::PCA() - Singular Value Decomposition (SVD)

+ princomp() - Spectral Decomposition

---

## Principal Components Regression

Use *principal components as predictors in a regression model* instead of using the original larger set of variables.

1. Transform $n \times p$ into $n \times M$ dimensions using Principal Components Analysis, where $M >> p$
2. Fit regression model using the $M$ predictors. 

---

## Principal Components Regression in R

Use ISLR::Hitters data set to predict baseball players **salaries** using **19 different variables**
```{r, eval = F}
library(ISLR) # data
library(pls) # https://cran.r-project.org/web/packages/pls/index.html
library(caret) # test/training split

data("Hitters")
hit_dat <- na.omit(Hitters) #remove missing values
set.seed(6)
split_train<- createDataPartition(Default$default, p = 0.8, list = F)

# perform PCR on training data with standardisation of predictors
# perform 10 fold cross validation to compute error for each possible value of M
set.seed(6)
pcr_fit <- pcr(Salary ~., data = hit_dat, scale = TRUE, subset = split_train, validation = "CV") 
summary(pcr_fit) #view fit
validationplot(pcr_fit, val.type="MSEP", legendpos = "top") #plot mean squared error; lowest = 6]

# make predictions on test data
pcr_pred <- predict(pcr_fit, hit_dat[-split_train, ], ncomp=6) # predictions
mean((pcr_pred-hit_dat[-split_train, "Salary"])^2) # assess MSE
```

---

## Preprocessing with Principal Components Analysis - not just for regression

Other classification and clustering methods can be adapted to use $n \times M$ matrix where there columns are the first $M << p$ principal component score vectors, rather than the full $n \times p$ data set. 


+ caret::preProcess()
    + [Caret Book](https://topepo.github.io/caret/pre-processing.html#pp)
    + [Jeff Leak's lecture](https://www.coursera.org/lecture/practical-machine-learning/preprocessing-with-principal-components-analysis-QU66P)

+ Do it manually yourself using **prcomp()** and then your classifier of choice *e.g.* **glm(family="binomial")**

+ Notes
    + Interpretation is harder as PCs are complex linear combinations of original variables
    + Outliers can have large effects on PCA - visualise first
    + Need to transform data? *e.g.* log scale

---

### Extra reading

+ Chapter 10.2 of James *et al.*, [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/) 
    + PCA for unsupervised learning
    
+ Chapter 6.3 of James *et al.*, [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/) 
    + Principal Components Regression

---

### References

+ James *et al.*, [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/)

**Slides**
+ xaringhan, xaringanthemer, remark.js, knitr, R Markdown
